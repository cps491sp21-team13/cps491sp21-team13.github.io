# Compressible Learning Agents for Autonomous Cyber-Physical Systems

### Researcher

Zachary Rowland

### Company Mentor

Matthew Clark, Principal Scientist

Galois

444 E 2nd Street

Dayton, OH 45402

## Project Overview

Deep learning has been demonstrated to be a versatile technique for learning a wide range of functions. The advantage of deep neural networks is the compounding non-linearity provided by each layer of neurons.
Deeper neural networks can provide better approximations of arbitrary functions similar to how higher-order polynomials can better approximate functions via Taylor series expansion.
However, unlike Taylor series expansion, we still lack a clear understanding of how the approximation works.
The goal of this project is to explore the correspondence between neural networks and the functions they approximate.
What kinds of simple neural networks can approximate simple functions like polynomials or sinusoids? Furthermore, how can these simple networks be combined to model more complex functions?
